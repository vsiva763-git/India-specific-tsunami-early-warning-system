# ğŸŒ IoT Team - Data Collection Component

## ğŸ“¡ Team Responsibility
Data collection from real-time APIs and public data sources to feed the tsunami warning system.

## ğŸ‘¥ Team Members
- **Lead:** [Your Name]
- **Contributors:** [Team Members]

---

## ğŸ“‹ Overview

The IoT team manages all data collection infrastructure, treating public APIs as remote sensors. We continuously monitor the Indian Ocean for earthquake activity, ocean conditions, and official advisories.

### **Data Sources:**
1. **USGS Earthquake API** - Real-time earthquake monitoring
2. **NOAA Tides API** - Sea level and tidal anomalies
3. **NOAA NDBC Buoys** - Wave height and ocean state
4. **INCOIS API** - Official Indian tsunami advisories
5. **GEBCO Bathymetry** - Ocean floor depth data

---

## ğŸ¯ Key Responsibilities

- âœ… Fetch data from 4 real-time APIs every 5 minutes
- âœ… Parse and validate incoming data
- âœ… Handle API failures and implement retry logic
- âœ… Clean and normalize data for model input
- âœ… Cache data efficiently to avoid rate limits
- âœ… Monitor data freshness and quality
- âœ… Provide structured data to model team
- âœ… Provide readable summaries to website team

---

## ğŸ“ Project Files You Own

```
src/data_collection/
â”œâ”€â”€ usgs_collector.py          â† USGS earthquake API
â”œâ”€â”€ noaa_tides_collector.py    â† NOAA tide gauge API
â”œâ”€â”€ noaa_buoys_collector.py    â† NOAA buoy API
â”œâ”€â”€ incois_collector.py        â† INCOIS advisory API
â””â”€â”€ bathymetry_loader.py       â† GEBCO bathymetry data

config/
â””â”€â”€ config.yaml               â† API configurations & endpoints

scripts/
â””â”€â”€ prepare_data.py           â† Data preparation utilities
```

---

## ğŸ”§ Technologies Used

- **Python 3.8+**
- **requests** - HTTP requests to APIs
- **pandas** - Data manipulation
- **numpy** - Numerical operations
- **loguru** - Logging

---

## ğŸ“Š Data Specifications

### **Earthquake Data**
```python
{
    'magnitude': float,           # 5.5-9.5
    'depth': float,              # kilometers
    'latitude': float,           # -20 to 30
    'longitude': float,          # 40 to 110
    'time': datetime,            # Event time
    'place': str,               # Location description
    'tsunami': bool             # Tsunami occurred?
}
```

### **Tides Data**
```python
{
    'station_id': str,
    'time': datetime,
    'water_level': float,        # meters
    'anomaly': float            # deviation from normal
}
```

### **Buoys Data**
```python
{
    'station_id': str,
    'time': datetime,
    'wave_height': float,        # meters
    'wave_period': float,        # seconds
    'wave_direction': float      # degrees
}
```

### **Bathymetry Data**
```python
{
    'latitude': float,
    'longitude': float,
    'depth': float              # negative = ocean, positive = land
}
```

---

## ğŸš€ How to Run Your Component

### **Test Data Collection**
```bash
# Fetch recent earthquakes
python -c "
from src.data_collection import USGSEarthquakeCollector
import yaml
config = yaml.safe_load(open('config/config.yaml'))
collector = USGSEarthquakeCollector(config)
earthquakes = collector.fetch_recent_earthquakes(hours=24)
print(f'Found {len(earthquakes)} earthquakes')
"

# Fetch tide data
python -c "
from src.data_collection import NOAATidesCollector
import yaml
config = yaml.safe_load(open('config/config.yaml'))
collector = NOAATidesCollector(config)
tides = collector.fetch_all_stations()
print(f'Fetched tides: {tides}')
"

# Fetch buoy data
python -c "
from src.data_collection import NOAABuoysCollector
import yaml
config = yaml.safe_load(open('config/config.yaml'))
collector = NOAABuoysCollector(config)
buoys = collector.fetch_all_buoys()
print(f'Fetched buoy data: {buoys}')
"
```

### **Prepare Training Data**
```bash
python scripts/prepare_data.py --download --prepare
```

---

## ğŸ“ˆ Performance Metrics

| Metric | Target | Status |
|--------|--------|--------|
| USGS Fetch Time | <2 sec | âœ… |
| NOAA Tides Fetch | <3 sec | âœ… |
| NOAA Buoys Fetch | <3 sec | âœ… |
| INCOIS Fetch | <2 sec | âœ… |
| Data Validation | 100% | âœ… |
| API Success Rate | >99% | âœ… |

---

## ğŸ§ª Testing Checklist

- [ ] USGS API returns valid earthquake data
- [ ] NOAA Tides API returns valid tide gauge data
- [ ] NOAA Buoys API returns valid wave data
- [ ] INCOIS API returns valid advisories
- [ ] All data validates correctly
- [ ] Error handling works for API failures
- [ ] Retry logic functions properly
- [ ] Data caching prevents rate limits
- [ ] Logging captures all important events

---

## ğŸ”— Integration Points

**Output to Model Team:**
- Clean, validated earthquake features
- Ocean condition features
- Spatial bathymetry data

**Output to Website Team:**
- Latest earthquake summary
- Current ocean conditions
- Active INCOIS advisories

---

## ğŸ“ Configuration

Edit `config/config.yaml` for your APIs:
```yaml
apis:
  usgs_earthquake:
    base_url: "https://earthquake.usgs.gov/fdsnws/event/1/query"
    min_magnitude: 5.5
    region:
      min_latitude: -20
      max_latitude: 30
      min_longitude: 40
      max_longitude: 110
    lookback_hours: 24
    
  noaa_tides:
    base_url: "https://api.tidesandcurrents.noaa.gov/api/prod/datagetter"
    stations:
      - "8443970"  # Example tide station
      
  noaa_buoys:
    base_url: "https://www.ndbc.noaa.gov/data/realtime2/"
    stations:
      - "23001"  # Arabian Sea
      - "23009"  # Bay of Bengal
```

---

## ğŸ› Troubleshooting

**API Connection Issues:**
- Check internet connectivity
- Verify API endpoints in config.yaml
- Check API rate limits
- Verify region boundaries match config

**Data Validation Issues:**
- Check data format matches specifications
- Verify missing value handling
- Check timestamp parsing

**Performance Issues:**
- Implement request caching
- Reduce fetch frequency if rate-limited
- Use threading for concurrent requests

---

## ğŸ“š API Documentation

- **USGS:** https://earthquake.usgs.gov/fdsnws/event/1/
- **NOAA Tides:** https://api.tidesandcurrents.noaa.gov/api/prod/
- **NOAA NDBC:** https://www.ndbc.noaa.gov/
- **INCOIS:** https://incois.gov.in/
- **GEBCO:** https://www.gebco.net/

---

## ğŸ¯ Next Steps

1. Verify all API connections work
2. Test error handling for API failures
3. Implement data quality checks
4. Optimize performance for large datasets
5. Document any API changes or updates
6. Prepare data export formats for model team

---

**Your team's work ensures the entire system has real, valid data to analyze!** ğŸŒŠğŸ“¡
