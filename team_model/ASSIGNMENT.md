# ðŸ“‹ Model Team Assignment Card

**Team Name:** Deep Learning & AI  
**Your Role:** Tsunami Risk Prediction Engine  
**Start Here:** `team_model/README.md`

---

## âœ¨ Your Unique Value

You're the **brain** of the system. Your model turns raw data into life-saving predictions.

---

## ðŸ“¦ Your Deliverables

### Phase 1: Architecture & Training (Week 1-2)
- [ ] Understand CNN-LSTM architecture
- [ ] Prepare training dataset (1000+ samples)
- [ ] Implement Focal Loss for class imbalance
- [ ] Train model on GPU (Colab recommended)
- [ ] Achieve >98% validation accuracy

### Phase 2: Optimization (Week 2-3)
- [ ] Optimize for CPU inference
- [ ] Achieve <200ms prediction time
- [ ] Test on new unseen data
- [ ] Implement confidence scoring
- [ ] Generate model metadata

### Phase 3: Integration (Week 3-4)
- [ ] Save model in Keras format
- [ ] Create inference pipeline
- [ ] Test with Website Team data format
- [ ] Performance profiling
- [ ] Documentation & examples

---

## ðŸ“Š Success Metrics

```
Metric                  Target          Status
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Training Accuracy       > 98%           â­•
Validation Accuracy     > 98%           â­•
Precision               100%            â­•
Recall                  > 95%           â­•
Inference Time (CPU)    < 200ms         â­•
Model Size              < 5 MB          â­•
AUC-ROC                 > 0.95          â­•
```

---

## ðŸ”§ Core Files to Modify

```
src/models/
â”œâ”€â”€ cnn_lstm_binary_model.py   â† Model architecture (EDIT THIS)
â”œâ”€â”€ data_preprocessor.py        â† Data normalization
â””â”€â”€ model_trainer.py            â† Training loop

models/
â””â”€â”€ tsunami_detection_binary_focal.keras  â† Your trained model

scripts/
â””â”€â”€ train_model.py              â† Command-line training
```

---

## ðŸš€ Quick Start

### **Option 1: Google Colab (Recommended)**
```
1. Open notebook:
   notebooks/Train_Tsunami_Binary_Focal_Loss_Kaggle.ipynb
   
2. In Colab:
   Runtime â†’ Change runtime type â†’ GPU
   
3. Run all cells
   Runtime â†’ Run all
   
4. Download trained model:
   tsunami_detection_binary_focal.keras
```

### **Option 2: Local Training**
```bash
# Prepare data
python scripts/prepare_data.py --sample --prepare

# Train model
python scripts/train_model.py \
  --epochs 50 \
  --batch-size 32 \
  --data-dir data/raw \
  --output-dir models/
```

---

## ðŸ“Š Model Architecture

```
Input Shape: (batch_size, 24, 32)
       â†“
CNN Block 1:
  Conv2D(32, 3x3) 
  â†’ MaxPooling(2x2) 
  â†’ Dropout(0.3)
       â†“
CNN Block 2:
  Conv2D(64, 3x3) 
  â†’ MaxPooling(2x2) 
  â†’ Dropout(0.3)
       â†“
LSTM Block 1:
  LSTM(128, return_seq=True)
  â†’ Dropout(0.3)
       â†“
LSTM Block 2:
  LSTM(64, return_seq=False)
  â†’ Dropout(0.3)
       â†“
Dense 1: Dense(128, relu) â†’ Dropout(0.3)
Dense 2: Dense(64, relu) â†’ Dropout(0.2)
Dense 3: Dense(32, relu)
       â†“
Output: Dense(1, sigmoid)
       â†“
Risk Probability [0-1]

Total Parameters: 185,729
Model Size: 2.3 MB
```

---

## ðŸ”— Integration Points

**Input from IoT Team:**
```json
{
  "earthquake": {
    "magnitude": 7.5,
    "depth_km": 30,
    "latitude": 8.5,
    "longitude": 93.2
  },
  "ocean": {
    "tide_anomaly": 0.45,
    "wave_height": 2.3,
    "wave_period": 12.5
  }
}
```

**Output to Website Team:**
```json
{
  "risk_probability": 0.85,
  "confidence": 0.92,
  "prediction": 1,
  "inference_time_ms": 145
}
```

---

## ðŸŽ¯ Weekly Checklist

**Week 1:**
- [ ] Read team documentation
- [ ] Understand CNN-LSTM architecture
- [ ] Review Focal Loss concept
- [ ] Set up training environment

**Week 2:**
- [ ] Prepare and validate dataset
- [ ] Build model architecture
- [ ] Implement training loop
- [ ] Train model and log metrics

**Week 3:**
- [ ] Achieve >98% accuracy
- [ ] Optimize for inference speed
- [ ] Save and test model loading
- [ ] Generate confidence scores

**Week 4:**
- [ ] Profile on CPU
- [ ] Test with Website Team
- [ ] Complete documentation
- [ ] Prepare for deployment

---

## ðŸ“š Learning Resources

### **CNN-LSTM Understanding:**
- https://keras.io/examples/timeseries/lstm_forecast_multistep/
- https://colah.github.io/posts/2014-07-Conv-Nets-Overview/
- https://colah.github.io/posts/2015-08-Understanding-LSTMs/

### **Class Imbalance & Focal Loss:**
- Paper: https://arxiv.org/abs/1708.02002
- https://machinelearningmastery.com/imbalanced-classification-with-python/

### **TensorFlow/Keras:**
- https://tensorflow.org/tutorials
- https://keras.io/api/

---

## ðŸŽ“ Training Data Details

### **Data Sources**
- NOAA Global Historical Tsunami Database
- USGS Earthquake Catalog (2000-present)
- GEBCO Bathymetry Database

### **Dataset Composition**
```
Total Samples:      ~1000-5000
Tsunami Events:     ~50-200   (5% positive class)
Non-Tsunami:        ~950-4800 (95% negative class)

Class Imbalance Challenge:
  â”œâ”€ Problem: Only 5% are actual tsunamis
  â”œâ”€ Solution: Focal Loss
  â””â”€ Result: 100% precision, 97% recall
```

### **Feature Engineering**
```
From earthquake data:
  â€¢ Magnitude (5.5-9.5)
  â€¢ Depth (0-700 km)
  â€¢ Location (lat/lon in Indian Ocean)
  â€¢ Time (for temporal patterns)

From ocean data:
  â€¢ Sea level anomalies
  â€¢ Wave height
  â€¢ Wave period
  
From bathymetry:
  â€¢ Ocean depth
  â€¢ Distance to coast
  â€¢ Seafloor topology
```

---

## âš ï¸ Common Challenges

**Challenge:** Class imbalance (5% positive)  
**Solution:** Focal Loss with gamma=2.0, alpha=0.25

**Challenge:** Model overfitting  
**Solution:** Dropout layers, early stopping, regularization

**Challenge:** Slow inference  
**Solution:** Reduce model size, quantization, optimize architecture

**Challenge:** Poor CPU performance  
**Solution:** Profile inference, optimize data loading, batch normalization

---

## ðŸ’¡ Pro Tips

1. **Use Focal Loss** - Critical for imbalanced data
2. **Add confidence scores** - Don't just predict class
3. **Profile early** - Check inference time on CPU
4. **Save model metadata** - Document performance metrics
5. **Test generalization** - Validate on held-out test set
6. **Implement batch processing** - Speed up inference

---

## ðŸ§ª Testing Checklist

- [ ] Model builds without errors
- [ ] Accepts correct input shapes
- [ ] Training loss decreases
- [ ] Validation accuracy improves
- [ ] Achieves >98% accuracy
- [ ] Inference produces values in [0,1]
- [ ] CPU inference <200ms
- [ ] Model saves and loads correctly
- [ ] Generalizes to new data
- [ ] Confidence scores are meaningful

---

**Your model's predictions save lives. Make them count!** ðŸ¤–ðŸŒŠ
